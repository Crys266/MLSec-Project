# -*- coding: utf-8 -*-
"""Copia di 06_explainability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YKy-s-ZuO52Iim7MLmbdb5dfaTKrsSev

# Explaining Machine Learning

Interpretability of Machine Learning models has recently become a relevant
 research direction to more thoroughly address and mitigate the issues of
 adversarial examples and to better understand the potential flaws of the
 most recent algorithm such as Deep Neural Networks.

In this tutorial, we explore different methods to compute
 *post-hoc* explanations, which consist of analyzing a trained model to
 understand which components such as features or training prototypes are
 more relevant during the decision (classification) phase.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](
https://colab.research.google.com/github/maurapintor/unica_mlsec_labs/blob/HEAD/06_explainability.ipynb)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture --no-stderr --no-display
# # NBVAL_IGNORE_OUTPUT
# 
# %pip install torch
# %pip install torchvision
# %pip install captum
# %pip install foolbox
# %pip install transformers
# %pip install datasets
# %pip install shap

import torch
from torchvision import models, transforms
import requests
from PIL import Image
import io
import numpy as np
from captum.attr import visualization as viz

"""## Explaining language models

In the second part of this tutorial we switch to the Natural Language Processing domain, focusing on transformer-based architectures. In particular we consider [BERT](https://arxiv.org/abs/1810.04805), a Large Language Model (LLM) pre-trained on a large text corpus which can be quickly fine-tuned on a wide range of downstream tasks, by attaching a different layer on top of the representation space.

We will rely to the [Hugging Face](https://huggingface.co) `transformers` library, which provides APIs, tools, and an open model zoo to easily download and load available models.

For this tutorial we select a text classification model that tries to infer emotions, among six different classes (sadness, joy, love, anger, fear, surprise). We start loading the [pre-trained model](https://huggingface.co/nateraw/bert-base-uncased-emotion) from Hugging Face.
"""

import transformers


# load the model and tokenizer
tokenizer = transformers.AutoTokenizer.from_pretrained(
    "nateraw/bert-base-uncased-emotion", use_fast=True
)
model = transformers.AutoModelForSequenceClassification.from_pretrained(
    "nateraw/bert-base-uncased-emotion"
).to(device)

# build a pipeline object to do predictions
transf_pipeline = transformers.pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    device=device,
    return_all_scores=True,  # observe the modelâ€™s behavior for all classes, not just the top output
)

# write a sentence
data = ["I am not happy, I'm sad rather than happy"]

# we first classify it
print(transf_pipeline(data)[0])

"""### SHAP

To provide local explanations we use [SHAP](https://github.com/shap/shap) (SHapley Additive exPlanations), a black-box unified framework based on Shapley values, a method from cooperative game theory that assigns each feature an importance value for a particular prediction.

> [[lundberg2017shap]](https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf) Scott M. Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, USA, 4768-4777.

Among the available techniques implemented in SHAP, we leverage the `Partition` method, which computes Shapley values recursively through a hierarchy of features by masking (i.e., removing) features from the input sample during
the computation of the explanation and evaluating the model outputs, capturing the correlation between input features.

SHAP fully support Hugging Face transformers library, so that we only have to pass the classifier pipeline to the explainer.
"""

import shap


masker = shap.maskers.Text(transf_pipeline.tokenizer)

print(masker.shape("I like this movie."))

print(masker(np.array([True] * 7), "I like this movie."))

print(masker(np.array([True, True, False, False, True, True, True]), "I like this movie."))

# wrap the classifier
transf_pipeline = shap.models.TransformersPipeline(transf_pipeline, rescale_to_logits=True)

# initialize the explainer
explainer = shap.Explainer(transf_pipeline)

# compute the attributions
shap_values = explainer(data)

"""### Visualize force plots

Once the SHAP values are computed, we can visualize feature attributions towards individual classes through force plots.

The base value is what the model outputs when the entire input text is masked, while $f_{class}(inputs)$ is the output of the model for the full original input. The SHAP values explain in an additive way how the impact of unmasking each word changes the model output from the base value (where the entire input is masked) to the final prediction value.
"""

shap.plots.text(shap_values)

"""### Global explanations

If we compute the local explanations on a set of samples and aggregate the results, we can obtain the most relevant features that influence the behavior of the model on that dataset.

We first load an emotion dataset from Hugging Face and randomly pick 10 samples, then compute SHAP values on them.
"""

import datasets
import pandas as pd


# load the emotion dataset
dataset = datasets.load_dataset("emotion", split="train")
data = pd.DataFrame({"text": dataset["text"], "emotion": dataset["label"]})

# get attributions for ten random samples
shap_values = explainer(data.sample(10, random_state=123)["text"])

"""### Visualize results

We can plot the most relevant features that impact a single class through a bar chart, after averaging the attributions for the selected class.
"""

red_shap_values = shap_values[:, :, "joy"].mean(0)

shap.plots.bar(red_shap_values)

# we can sort the bar chart in decending order
shap.plots.bar(red_shap_values, order=shap.Explanation.argsort)

# ...or acending order
shap.plots.bar(red_shap_values, order=shap.Explanation.argsort.flip)