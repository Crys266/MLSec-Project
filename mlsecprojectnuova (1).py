# -*- coding: utf-8 -*-
"""MLSecProjectNuova.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfUhJzoVx_j-jdmHVl-qxhNSTquG9zIA

# Explainability Project for Machine Learning Security
Questo notebook esegue spiegazioni di modelli di classificazione del testo utilizzando SHAP, transformers-interpret e Captum.

### **Installing Requirements**
"""

!git clone https://github.com/Crys266/MLSec-Project.git

import sys
sys.path.append('/content/MLSec-Project')

!pip install numpy==2.0.2 seaborn==0.13.2 captum==0.7.0 shap==0.47.0 transformers==4.41.2 transformers-interpret==0.10.0 reportlab flask-compress ipywidgets lime

from transformers import BertTokenizer, BertForSequenceClassification
from sequence_explainer import NewSequenceClassificationExplainer
import shap
import torch
import torch.nn.functional as F
import transformers
from captum.attr import visualization as viz
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import random
import pandas as pd
from lime.lime_text import LimeTextExplainer
import re

"""Select seed (si puÃ² anche non mettere)"""

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

"""### Caricamento del Modello e del Tokenizer
In questa sezione, carichiamo il modello di classificazione del testo e il tokenizer utilizzando la libreria Transformers.
"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertForSequenceClassification.from_pretrained("textattack/bert-base-uncased-imdb").to(device)
tokenizer = BertTokenizer.from_pretrained("textattack/bert-base-uncased-imdb")
model.eval()
set_seed(42)

"""### **Text Examples**
In questa sezione, definiamo alcuni testi di esempio da utilizzare per le spiegazioni e li classifichiamo.
"""

# Sample text inputs to cover all 5 classes
sentences = [
    "I really love fantasy movies, they are so exciting!",
    "that movie was awesome.",
    "that movie was good.",
    "that movie was terrible.",
    "The film was a complete waste of time.",
    "The film was a complete waste of time",
    "The film was acomplete waste of time.",
    "What a great movie! ... if you have no taste.",
    "I absolutely loved this movie! I hate that people don't like this movie.",
    "I am not sure how I feel about this film.",
]

print(f"Text inputs: {sentences}")

# **TOKENIZZAZIONE CORRETTA**
inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors="pt")  # Convertiamo i testi in tensori
# **SPOSTIAMO I TENSORI SULLA STESSA DEVICE DEL MODELLO**
inputs = {key: value.to(device) for key, value in inputs.items()}

outputs = model(**inputs)  # Otteniamo le predizioni
# **ESTRAGGIAMO LE PREDIZIONI**
logits = outputs.logits  # Otteniamo i logits (punteggi grezzi)
probabilities = F.softmax(logits, dim=-1)  # Applichiamo softmax per ottenere probabilitÃ 
predicted_classes = torch.argmax(probabilities, dim=-1).tolist()  # Troviamo la classe con probabilitÃ  piÃ¹ alta

wb_attributions = []
bb_attributions = []

# Definizione delle classi, supponendo un mapping delle classi del modello
class_labels = ["Negative", "Positive"]
# Stampa delle classi predette con probabilitÃ 
for i, pred in enumerate(predicted_classes):
    predicted_class = class_labels[pred]  # Converte l'indice nella label della classe
    probability = probabilities[i][pred].item()  # Estrae la probabilitÃ  della classe predetta
    print(f"Sample {i}: \"{sentences[i]}\" -> Predicted Class: {predicted_class} (Probability: {probability:.4f})")

"""### **Spiegazione Black-box con SHAP**
In questa sezione, utilizziamo SHAP per calcolare le attribuzioni delle spiegazioni black-box.
"""

# Metodo SHAP
print("\nComputing black-box explanation with SHAP...")

# Costruzione pipeline
transf_pipeline = transformers.pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1,
    top_k=None
)

# Wrapping SHAP
transf_pipeline = shap.models.TransformersPipeline(transf_pipeline, rescale_to_logits=True)
explainer = shap.Explainer(transf_pipeline)

# Esecuzione SHAP
shap_values = explainer(sentences)

# Elaborazione SHAP
for i, value in enumerate(shap_values):
    sentence = sentences[i]
    predicted_class_index = predicted_classes[i]
    predicted_class_label = class_labels[predicted_class_index]
    prob = probabilities[i][predicted_class_index].item()

    vals = torch.tensor(value.values, dtype=torch.float64)  # [n_token, n_classi]
    norm = torch.norm(vals[:, predicted_class_index])
    norm = norm if norm != 0 else 1e-8
    normed = vals[:, predicted_class_index] / norm  # [n_token]

    tokens = list(value.data)
    if tokens[0] == "": tokens[0] = "[CLS]"
    if tokens[-1] == "": tokens[-1] = "[SEP]"

    shap_attributions = list(zip(tokens, normed.tolist()))

    # Inverti se classe negativa (indice 0)
    if predicted_class_index == 0:
        shap_attributions = [(tok, -score) for tok, score in shap_attributions]

    # Salva in all_attributions
    bb_attributions.append({
        "sentence": sentence,
        "explain_type": "shap",
        "attributions": shap_attributions,
        "pred_prob": prob,
        "pred_class": predicted_class_label
    })

    # Output di debug (facoltativo)
    print(f"\nWord attributions for: \"{sentence}\"")
    print(f"Attribution type: SHAP for predicted class â†’ {predicted_class_label} (p = {prob:.4f})")
    for token, score in shap_attributions:
        print(f"{token:>10} : {score:+.4f}")

"""SHAP Visualization"""

# Visualization with SHAP
def shap_visualization(texts, shap_values):
    shap.initjs()
    for i, text in enumerate(texts):
        print(f"\nSHAP Visualization for: {text}")
        shap.plots.text(shap_values[i], display=True)

shap_visualization(sentences, shap_values)

"""### **Spiegazione Black-box con LIME**
In questa sezione, utilizziamo LIME per calcolare le attribuzioni delle spiegazioni black-box.
"""

# Metodo LIME
print("\nComputing black-box explanation with LIME...")

saved_explanations = []

# Prediction function for LIME
def lime_predict(texts):
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    probs = F.softmax(outputs.logits, dim=-1).cpu().numpy()
    return probs

# Custom tokenization function using regex that extracts words and punctuation
def custom_tokenizer(text):
    # Questa regex estrae sequenze di caratteri alfanumerici (inclusi gli apostrofi)
    # oppure ogni segno di punteggiatura individualmente
    return re.findall(r"\w+|[^\w\s]", text)

# LIME Explainer: utilizza la funzione custom_tokenizer
lime_explainer = LimeTextExplainer(class_names=class_labels,
                                   split_expression=custom_tokenizer,
                                   bow=False,
                                   char_level=False,
                                   kernel_width=5,  # prova a variare questo valore
                                   feature_selection='forward_selection',  # oppure 'auto' o 'highest_weights'
                                   random_state=42  # per la riproducibilitÃ 
                                   )

print("=== LIME Explanation ===")
for i, sentence in enumerate(sentences):
    model_tokens = tokenizer("that movie was terrible.", return_tensors="pt")["input_ids"]
    custom_tokens = custom_tokenizer("that movie was terrible.")
    print(f"Model tokens: {model_tokens}")
    print(f"Custom tokens: {custom_tokens}")

    predicted_class_index = predicted_classes[i]
    predicted_class_label = class_labels[predicted_class_index]
    prob = probabilities[i][predicted_class_index].item()

    explanation = lime_explainer.explain_instance(
        sentence,
        lime_predict,
        num_features=100,
        labels=[predicted_class_index],
        num_samples=5000
    )
    # Save the explanation so we can use it later without re-running explain_instance()
    saved_explanations.append((sentence, explanation))

    # Recupera le attribuzioni usando i token generati dalla funzione custom_tokenizer
    lime_dict = dict(explanation.as_list(label=predicted_class_index))
    tokens = custom_tokenizer(sentence)
    # Aggiungiamo manualmente i token speciali se necessario
    lime_aligned = [("[CLS]", 0.0)] + [(tok, lime_dict.get(tok, 0.0)) for tok in tokens] + [("[SEP]", 0.0)]

    # Se la classe predetta Ã¨ Negative ed Ã¨ richiesta un'inversione del segno sulle attribuzioni:
    if predicted_class_index == 0:
        lime_aligned = [(tok, -score) for tok, score in lime_aligned]

    bb_attributions.append({
        "sentence": sentence,
        "explain_type": "lime",
        "attributions": lime_aligned,
        "pred_prob": prob,
        "pred_class": predicted_class_label
    })

    print(f"\nWord attributions for: \"{sentence}\"")
    print(f"Attribution type: LIME for predicted class â†’ {predicted_class_label} (p = {prob:.4f})")
    for token, score in lime_aligned:
        print(f"{token:>10} : {score:+.4f}")

"""Visualizzazione LIME:"""

# --- Additional block: Display explanations in the notebook without re-running explain_instance() ---
print("\n=== Display explanations in notebook ===")
for sentence, exp in saved_explanations:
    exp.show_in_notebook(text=sentence)

"""### **Spiegazione White-box con transformers-interpret**
In questa sezione, utilizziamo transformers-interpret per calcolare le attribuzioni delle spiegazioni white-box.
"""

# Memorizziamo le attribuzioni per ogni frase e tipo di spiegazione
explain_types = ["ig", "lig", "gs"]
for explain_type in explain_types:
    explainer = NewSequenceClassificationExplainer(model, tokenizer, attribution_type=explain_type)
    for i, sentence in enumerate(sentences):
        predicted_class_index = predicted_classes[i]
        predicted_class_label = class_labels[predicted_class_index]
        prob = probabilities[i][predicted_class_index].item()

        attributions = explainer(sentence)
        if predicted_class_index == 0:
            attributions = [(tok, -score) for tok, score in attributions]

        # Salva le attribuzioni in una lista
        wb_attributions.append({
            "sentence": sentence,
            "explain_type": explain_type,
            "attributions": attributions,
            "pred_prob": probabilities[0, predicted_classes[0]].item(),
            "pred_class": class_labels[predicted_class_index],
            "delta": explainer.attributions.delta.item(),
            "sensitivity": explainer.attributions.sensitivity.item()
        })
        # Print the attributions for each token in the sentence.
        print(f"\nWord attributions for: \"{sentence}\"")
        print(f"Attribution type: '{explain_type}' for predicted class â†’ {predicted_class_label} (p = {prob:.4f})")
        print(f"Convergence Delta for: {explainer.attributions.delta.item():.4f}")
        print(f"Sensitivity for: {explainer.attributions.sensitivity.item():.4f}")
        for token, score in attributions:
            print(f"{token:>10} : {score:.4f}")
        print(f"-------------------------------------")

"""## **Visualizzazione dei Risultati**
In questa sezione, visualizziamo i risultati delle spiegazioni calcolate utilizzando SHAP, transformers-interpret e Captum.

## Funzioni di Plotting
In questa sezione, definiamo le funzioni per plottare le attribuzioni calcolate.

Black-Box:
"""

for entry in bb_attributions:
    print(f"\nAttributions for method: \"{entry['explain_type']}\"")
    print(entry['attributions'])
    tokens = [token for token, _ in entry["attributions"]]
    scores = [score for _, score in entry["attributions"]]
    vis_data_records= [viz.VisualizationDataRecord(
            word_attributions=scores,
            pred_prob=entry["pred_prob"],
            pred_class=entry["pred_class"],
            true_class=entry["pred_class"],  # Supponendo che la classe predetta sia corretta
            attr_class=entry["pred_class"],
            attr_score=sum(scores),
            raw_input_ids=tokens,
            convergence_score=0.0,  # Non calcolato in questo contesto
        )]
    # Visualizza le attribuzioni
    viz.visualize_text(vis_data_records)

"""White-Box:"""

for entry in wb_attributions:
    print(f"\nAttributions for method: \"{entry['explain_type']}\"")
    print(entry['attributions'])
    tokens = [token for token, _ in entry["attributions"]]
    scores = [score for _, score in entry["attributions"]]
    vis_data_records= [viz.VisualizationDataRecord(
            word_attributions=scores,
            pred_prob=entry["pred_prob"],
            pred_class=entry["pred_class"],
            true_class=entry["pred_class"],  # Supponendo che la classe predetta sia corretta
            attr_class=entry["pred_class"],
            attr_score=sum(scores),
            raw_input_ids=tokens,
            convergence_score=0.0,  # Non calcolato in questo contesto
        )]
    # Visualizza le attribuzioni
    viz.visualize_text(vis_data_records)

"""### **Visualizzazione Plot**
In questa sezione plottiamo i risultati dei metodi di explanation
"""

from matplotlib.colors import LinearSegmentedColormap

def plot_exp_per_classes(all_attributions):
    for entry in all_attributions:
        sentence = entry["sentence"]
        explain_type = entry['explain_type']
        tokens = [token for token, _ in entry["attributions"]]
        attributions = [attribution for _, attribution in entry["attributions"]]
        class_name = entry["pred_class"]

        if len(tokens) != len(attributions):
            print(f"âš ï¸ Mismatch in token/attribution length for {explain_type} on sentence: '{sentence}'")
            continue

        plt.figure(figsize=(10, 6))

        # Costruzione colormap personalizzata: rosso â†” bianco â†” verde
        custom_cmap = LinearSegmentedColormap.from_list(
            "custom_red_white_green",
            ["#f00707", "white", "#07f080"],
            N=256
        )
        # Normalizzazione centrata su zero
        min_attr, max_attr = min(attributions), max(attributions)
        if min_attr == max_attr:
            min_attr -= 1e-6
            max_attr += 1e-6
        vabs = max(abs(min_attr), abs(max_attr))
        normed = [(val + vabs) / (2 * vabs) for val in attributions]  # mappati in [0, 1]
        # Applica la colormap personalizzata
        colors = [custom_cmap(val) for val in normed]

        # Plot
        sns.barplot(x=attributions, y=tokens, palette=colors, edgecolor="black", linewidth=0.5)
        plt.title(f"Explanation '{explain_type}' for Class: {class_name}\nText: {sentence}")
        plt.xlabel("Attribution Value")
        plt.ylabel("Tokens")
        plt.tight_layout()
        plt.show()


plot_exp_per_classes(bb_attributions)
plot_exp_per_classes(wb_attributions)

"""## Creazione e Salvataggio della Tabella di Confronto
In questa sezione, creiamo e salviamo una tabella di confronto delle attribuzioni calcolate.
"""

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import os
from collections import defaultdict
from IPython.display import display

# Path
pdf_path = "/mnt/data/attribution_comparison.pdf"
os.makedirs(os.path.dirname(pdf_path), exist_ok=True)

# Group by sentence
grouped = defaultdict(lambda: defaultdict(list))
for entry in bb_attributions:
    sentence = entry["sentence"]
    method = entry["explain_type"]
    grouped[sentence][method] = entry["attributions"]

# Funzione per normalizzare i token
def normalize_token(tok):
    return tok.lower().strip()

# Generate the PDF
with PdfPages(pdf_path) as pdf:
    for sentence, methods_dict in grouped.items():
        method_dicts = {}

        # Usa SHAP come riferimento per l'ordine dei token (o il primo metodo che trovi)
        reference_method = list(methods_dict.keys())[0]
        reference_tokens = [normalize_token(tok) for tok, _ in methods_dict[reference_method]]

        token_set = set(reference_tokens)

        # Aggiungi anche token nuovi degli altri metodi (se presenti)
        for method, attribs in methods_dict.items():
            for tok, _ in attribs:
                tok_norm = normalize_token(tok)
                if tok_norm not in token_set:
                    reference_tokens.append(tok_norm)
                    token_set.add(tok_norm)

        for method, attribs in methods_dict.items():
            norm_attribs = defaultdict(list)
            for tok, val in attribs:
                tok_norm = normalize_token(tok)
                norm_attribs[tok_norm].append(val)
            method_dicts[method.upper()] = norm_attribs

        all_tokens = reference_tokens  # ðŸ‘ˆ usa l'ordine dei token di riferimento

        # Build the DataFrame
        data = {"Token": all_tokens}
        for method, norm_attribs in method_dicts.items():
            values = []
            for tok in all_tokens:
                if tok in norm_attribs:
                    values.append(round(sum(norm_attribs[tok]) / len(norm_attribs[tok]), 5))
                else:
                    values.append(0)
            data[method] = values

        df = pd.DataFrame(data)

        # Show in notebook
        print(f"\nAttribution Comparison for: \"{sentence}\"")
        display(df)

        # Plot
        fig, ax = plt.subplots(figsize=(10, 0.4 * len(df) + 1))
        ax.axis("off")
        ax.set_title(f"Word Attribution Comparison\nSentence: \"{sentence}\"", fontsize=12, pad=20)
        table = ax.table(cellText=df.values,
                         colLabels=df.columns,
                         cellLoc='center',
                         loc='center',
                         colColours=["#f2f2f2"] * len(df.columns))
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1, 1.5)

        pdf.savefig(fig, bbox_inches='tight')
        plt.close()

pdf_path

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import os
from matplotlib import rcParams
from collections import defaultdict
from IPython.display import display

# Set path and ensure directory exists
pdf_path = "/mnt/data/attribution_comparison.pdf"
os.makedirs(os.path.dirname(pdf_path), exist_ok=True)

# Group all attributions by sentence
grouped = defaultdict(lambda: defaultdict(list))
for entry in wb_attributions:
    sentence = entry["sentence"]
    method = entry["explain_type"]
    grouped[sentence][method] = entry["attributions"]

# Generate the PDF with rounded values
with PdfPages(pdf_path) as pdf:
    for sentence, methods_dict in grouped.items():
        tokens = [tok for tok, _ in list(methods_dict.values())[0]]
        data = {"Token": tokens}

        for method, attribs in methods_dict.items():
            values = [round(val, 5) for _, val in attribs]  # rounded attribution values
            data[method.upper()] = values

        df = pd.DataFrame(data)

        # Show table in notebook
        print(f"\nAttribution Comparison for: \"{sentence}\"\n")
        display(df)

        # Plot
        fig, ax = plt.subplots(figsize=(10, 0.4 * len(df) + 1))
        ax.axis("off")
        ax.set_title(f"Word Attribution Comparison\nSentence: \"{sentence}\"", fontsize=12, pad=20)
        table = ax.table(cellText=df.values,
                         colLabels=df.columns,
                         cellLoc='center',
                         loc='center',
                         colColours=["#f2f2f2"] * len(df.columns))
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1, 1.5)

        pdf.savefig(fig, bbox_inches='tight')
        plt.close()

pdf_path

"""### **Conversione della Tabella in PDF**
In questa sezione, convertiamo la tabella di confronto in un file PDF.

# Aversarial Attack with TextAttack

We now try to fool the model using an adversarial library, i.e. TextAttack
"""

!pip install textattack

from textattack.attack_recipes import TextFoolerJin2019
from textattack.models.wrappers import HuggingFaceModelWrapper
import nltk
nltk.download('averaged_perceptron_tagger_eng')


# Wrappa il modello per TextAttack
model_wrapper = HuggingFaceModelWrapper(model, tokenizer)

# Scegli un attacco, es. TextFooler
attack = TextFoolerJin2019.build(model_wrapper)

# Test su una recensione
text = "This movie was absolutely terrible, I hated it."
attack_result = attack.attack(text, 0)
print(attack_result)

from textattack.attack_recipes import TextBuggerLi2018

# Wrappa il modello per TextAttack
model_wrapper = HuggingFaceModelWrapper(model, tokenizer)

# Configura TextBugger
attack = TextBuggerLi2018.build(model_wrapper)

# Test con una frase negativa
text = "The film was terrible."
ground_truth_label = 0  # Supponiamo che "0" sia NEGATIVO

# Esegui l'attacco
attack_result = attack.attack(text, ground_truth_label)
print(attack_result)

"""### **Substitution-based Attacks**

Sostituire parole con sinonimi, caratteri simili (homoglyphs), o altre manipolazioni linguistiche senza cambiare il significato del testo (ad esempio: "good" â†’ "g00d", "happy" â†’ "h@ppy").
"""

import textattack
from textattack.transformations import WordSwapHomoglyphSwap, WordSwapQWERTY, WordSwapRandomCharacterSubstitution
from textattack.augmentation import Augmenter
import torch.nn.functional as F

# ðŸ”¹ Testi negativi da attaccare
texts = [
    "The film was a complete waste of time.",  # Negativo
    "I hated every second of this movie.",  # Negativo
    #"The acting was awful and the plot made no sense.",  # Negativo
    #"This was one of the worst movies I have ever seen.",  # Negativo
]

predicted_classes_ad = []
predicted_labels_ad = []
probabilities_ad = []

# ðŸ”¹ Attacco: introduce caratteri speciali per ingannare BERT
transformation = textattack.transformations.CompositeTransformation([
    WordSwapHomoglyphSwap(),  # Cambia lettere con caratteri Unicode simili (e.g., a â†’ É‘)
    WordSwapQWERTY(),  # Introduce errori tipici della tastiera
    WordSwapRandomCharacterSubstitution(),  # Sostituisce caratteri con simboli simili (e.g., o â†’ 0)
])

augmenter = Augmenter(transformation=transformation, pct_words_to_swap=0.4, transformations_per_example=10)

# ðŸ”¹ Funzione per ottenere la predizione
def classify_review(texts):
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    inputs = {key: value.to(device) for key, value in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    probabilities = F.softmax(logits, dim=-1)
    predicted_classes = torch.argmax(probabilities, dim=-1).tolist()
    return predicted_classes, probabilities

# ðŸ”¹ Classificazione testi originali
original_predictions, original_probs = classify_review(texts)

# ðŸ”¹ Generazione e selezione della migliore versione
best_modifications = []
for i, text in enumerate(texts):
    modified_versions = augmenter.augment(text)  # Generiamo piÃ¹ perturbazioni
    modified_predictions, modified_probs = classify_review(modified_versions)
    # ðŸ”¹ Selezioniamo la versione con la probabilitÃ  piÃ¹ alta di essere positiva
    best_index = max(range(len(modified_versions)), key=lambda i: modified_probs[i][-1].item())
    best_modifications.append(modified_versions[best_index])


# ðŸ”¹ Classificazione dei testi modificati scelti
modified_predictions, modified_probs = classify_review(best_modifications)
predicted_classes_ad.append(modified_predictions)
probabilities_ad += modified_probs

# ðŸ”¹ Mappa delle classi
class_labels = ["Negative", "Positive"]

# ðŸ”¹ Stampa dei risultati
for i, (orig_text, mod_text) in enumerate(zip(texts, best_modifications)):
    orig_pred = class_labels[original_predictions[i]]
    mod_pred = class_labels[modified_predictions[i]]
    predicted_labels_ad.append(mod_pred)

    print(f"ðŸ”¹ Original: \"{orig_text}\" -> {orig_pred} (Prob: {original_probs[i][original_predictions[i]]:.4f})")
    print(f"ðŸ”¥ Modified: \"{mod_text}\" -> {mod_pred} (Prob: {modified_probs[i][modified_predictions[i]]:.4f})")
    print("-------------------------------------------------")

"""Nuova versione dell'attacco per forzare una classe diversa"""

from textattack.transformations import WordSwapHomoglyphSwap, WordSwapQWERTY, WordSwapRandomCharacterSubstitution
from textattack.augmentation import Augmenter
import torch
import torch.nn.functional as F

# ðŸ”¹ Impostazioni
MAX_ITERATIONS = 3  # Numero massimo di cicli successivi di attacco
MODIFICATIONS_PER_ITER = 10  # Tentativi per ciclo
TARGET_CLASS_INDEX = 1  # Supponiamo di voler forzare verso "Positive"

# ðŸ”¹ Trasformazioni dell'attacco
transformation = textattack.transformations.CompositeTransformation([
    WordSwapHomoglyphSwap(),
    WordSwapQWERTY(),
    WordSwapRandomCharacterSubstitution(),
])
augmenter = Augmenter(
    transformation=transformation,
    pct_words_to_swap=0.4,
    transformations_per_example=MODIFICATIONS_PER_ITER
)

# ðŸ”¹ Funzione di classificazione
def classify_review(texts):
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
    inputs = {key: value.to(device) for key, value in inputs.items()}
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    probabilities = F.softmax(logits, dim=-1)
    predicted_classes = torch.argmax(probabilities, dim=-1).tolist()
    return predicted_classes, probabilities

# ðŸ”¹ Classificazione originale
original_predictions, original_probs = classify_review(texts)
class_labels = ["Negative", "Positive"]

# ðŸ”¹ Attacco iterativo controllato
final_modifications = []
for i, text in enumerate(texts):
    current_text = text
    orig_class = original_predictions[i]

    for iteration in range(MAX_ITERATIONS):
        modified_versions = augmenter.augment(current_text)
        pred_classes, pred_probs = classify_review(modified_versions)

        # Se tra le modifiche c'Ã¨ un cambio di classe, prendi la prima che cambia
        for j, pred_class in enumerate(pred_classes):
            if pred_class != orig_class:
                final_modifications.append(modified_versions[j])
                break
        else:
            # Nessuna modifica ha cambiato la classe â†’ scegli quella con piÃ¹ probabilitÃ  target
            best_index = max(range(len(modified_versions)), key=lambda j: pred_probs[j][TARGET_CLASS_INDEX].item())
            current_text = modified_versions[best_index]
            if iteration == MAX_ITERATIONS - 1:
                final_modifications.append(current_text)
            continue
        break

# ðŸ”¹ Classificazione finale dei testi modificati
modified_predictions, modified_probs = classify_review(final_modifications)
predicted_classes_ad.append(modified_predictions)
probabilities_ad += modified_probs

# ðŸ”¹ Stampa dei risultati
results = []
for i, (orig_text, mod_text) in enumerate(zip(texts, final_modifications)):
    orig_pred = class_labels[original_predictions[i]]
    mod_pred = class_labels[modified_predictions[i]]
    predicted_labels_ad.append(mod_pred)

    print(f"\nðŸ”¹ Original: \"{orig_text}\" -> {orig_pred} (Prob: {original_probs[i][original_predictions[i]]:.4f})")
    print(f"ðŸ”¥ Modified: \"{mod_text}\" -> {mod_pred} (Prob: {modified_probs[i][modified_predictions[i]]:.4f})")
    print("-------------------------------------------------")
    results.append((orig_text, mod_text, orig_pred, mod_pred, original_probs[i], modified_probs[i]))

"""Explanation of Adversarial Samples:"""

ad_wb_attributions = []
ad_bb_attributions = []

ad_sentences = best_modifications + final_modifications
predicted_classes_ad = [item for sublist in predicted_classes_ad for item in sublist]
print("All Adversarial Samples: ", ad_sentences)
print("Predicition for AD: ", predicted_classes_ad)
print("Predicition Prob for AD: ", probabilities_ad)

"""White-Box:"""

# Memorizziamo le attribuzioni per ogni frase e tipo di spiegazione
explain_types = ["ig", "lig", "gs"]
for explain_type in explain_types:
    explainer = NewSequenceClassificationExplainer(model, tokenizer, attribution_type=explain_type)
    for i, sentence in enumerate(ad_sentences):
        predicted_class_index = predicted_classes_ad[i]
        predicted_class_label = predicted_labels_ad[i]
        prob = probabilities_ad[i][predicted_class_index].item()

        attributions = explainer(sentence)
        if predicted_class_index == 0:
            attributions = [(tok, -score) for tok, score in attributions]

        # Salva le attribuzioni in una lista
        ad_wb_attributions.append({
            "sentence": sentence,
            "explain_type": explain_type,
            "attributions": attributions,
            "pred_prob": probabilities[0, predicted_classes[0]].item(),
            "pred_class": class_labels[predicted_class_index],
            "delta": explainer.attributions.delta.item(),
            "sensitivity": explainer.attributions.sensitivity.item()
        })
        # Print the attributions for each token in the sentence.
        print(f"\nWord attributions for: \"{sentence}\"")
        print(f"Attribution type: {explain_type} for predicted class â†’ {predicted_class_label} (p = {prob:.4f})")
        for token, score in attributions:
            print(f"{token:>10} : {score:.4f}")
        print(f"Convergence Delta for {explain_type}: {explainer.attributions.delta.item():.4f}")
        print(f"Sensitivity for {explain_type}: {explainer.attributions.sensitivity.item():.4f}")
        print(f"-------------------------------------")

"""Black-Box:

LIME:
"""

print("=== LIME Explanation ===")
for i, sentence in enumerate(ad_sentences):
    predicted_class_index = predicted_classes_ad[i]
    predicted_class_label = predicted_labels_ad[i]
    prob = probabilities_ad[i][predicted_class_index].item()

    explanation = lime_explainer.explain_instance(
        sentence,
        lime_predict,
        num_features=100,
        labels=[predicted_class_index],
        num_samples=2000
    )
    # Save the explanation so we can use it later without re-running explain_instance()
    saved_explanations.append((sentence, explanation))

    # Recupera le attribuzioni usando i token generati dalla funzione custom_tokenizer
    lime_dict = dict(explanation.as_list(label=predicted_class_index))
    tokens = custom_tokenizer(sentence)
    # Aggiungiamo manualmente i token speciali se necessario
    lime_aligned = [("[CLS]", 0.0)] + [(tok, lime_dict.get(tok, 0.0)) for tok in tokens] + [("[SEP]", 0.0)]

    # Se la classe predetta Ã¨ Negative ed Ã¨ richiesta un'inversione del segno sulle attribuzioni:
    if predicted_class_index == 0:
        lime_aligned = [(tok, -score) for tok, score in lime_aligned]

    ad_bb_attributions.append({
        "sentence": sentence,
        "explain_type": "lime",
        "attributions": lime_aligned,
        "pred_prob": prob,
        "pred_class": predicted_class_label
    })

    print(f"\nWord attributions for: \"{sentence}\"")
    print(f"Attribution type: LIME for predicted class â†’ {predicted_class_label} (p = {prob:.4f})")
    for token, score in lime_aligned:
        print(f"{token:>10} : {score:+.4f}")

"""SHAP:"""

# Metodo SHAP
print("\nComputing black-box explanation with SHAP...")

# Esecuzione SHAP
explainer = shap.Explainer(transf_pipeline)
shap_values = explainer(ad_sentences)

# Elaborazione SHAP
for i, value in enumerate(shap_values):
    sentence = sentences[i]
    predicted_class_index = predicted_classes_ad[i]
    predicted_class_label = predicted_labels_ad[i]
    prob = probabilities_ad[i][predicted_class_index].item()

    vals = torch.tensor(value.values, dtype=torch.float64)  # [n_token, n_classi]
    norm = torch.norm(vals[:, predicted_class_index])
    norm = norm if norm != 0 else 1e-8
    normed = vals[:, predicted_class_index] / norm  # [n_token]

    tokens = list(value.data)
    if tokens[0] == "": tokens[0] = "[CLS]"
    if tokens[-1] == "": tokens[-1] = "[SEP]"

    shap_attributions = list(zip(tokens, normed.tolist()))

    # Inverti se classe negativa (indice 0)
    if predicted_class_index == 0:
        shap_attributions = [(tok, -score) for tok, score in shap_attributions]

    # Salva in all_attributions
    ad_bb_attributions.append({
        "sentence": sentence,
        "explain_type": "shap",
        "attributions": shap_attributions,
        "pred_prob": prob,
        "pred_class": predicted_class_label
    })

    # Output di debug (facoltativo)
    print(f"\nWord attributions for: \"{sentence}\"")
    print(f"Attribution type: SHAP for predicted class â†’ {predicted_class_label} (p = {prob:.4f})")
    for token, score in shap_attributions:
        print(f"{token:>10} : {score:+.4f}")

"""Plot:"""

plot_exp_per_classes(ad_bb_attributions)
plot_exp_per_classes(ad_wb_attributions)

"""### **Insertion attacks & Deletion attacks (DA FARE)**

Insertion Attacks: Inserire parole o frasi che non alterano il significato ma che possono disturbare il modello, portandolo a una classificazione errata.
Deletion Attacks: Rimuovere parole che sono fondamentali per la corretta classificazione ma che non cambiano il significato apparente della frase.
"""

import torch
import torch.nn.functional as F
from transformers import BertTokenizer, BertForSequenceClassification

# Caricamento del tokenizer e del modello pre-addestrato BERT per la classificazione del sentiment
tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')

# Inizializzazione del dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Funzione per inserire parole sarcastiche positive e osservare come cambia la classificazione
def sarcasm_insertion_attack(texts):
    # Lista di parole sarcastiche positive e frasi per enfatizzare il sarcasmo
    sarcastic_words = ["amazing", "fantastic", "incredible", "unbelievable", "brilliant", "spectacular", "excellent"]
    sarcastic_phrases = [
        "What a disaster, absolutely brilliant!",
        "Great movie, if you love boredom.",
        "Absolutely loved it, couldn't have been worse.",
        "Such a masterpiece, if you enjoy pain.",
        "This movie was perfect, just as I imagined it would be... horrible."
    ]

    # Elaborazione dei testi
    for text in texts:
        # Tokenizzazione del testo originale
        inputs = tokenizer([text], padding=True, truncation=True, return_tensors="pt")
        inputs = {key: value.to(device) for key, value in inputs.items()}

        # Classificazione del testo originale
        with torch.no_grad():
            outputs = model(**inputs)
        logits = outputs.logits
        probabilities = F.softmax(logits, dim=-1)
        predicted_class = torch.argmax(probabilities, dim=-1).tolist()[0]

        # Stampa la previsione originale
        class_labels = ["Negative", "Slightly Negative", "Neutral", "Slightly Positive", "Positive"]
        print(f"Original Text: \"{text}\" -> Predicted Class: {class_labels[predicted_class]} (Probability: {probabilities[0][predicted_class].item():.4f})")

        # Inserimento di parole sarcastiche positive
        # 1. Inserimento di parole sarcastiche positive
        text_with_insertion = text + " " + " ".join(sarcastic_words[:3])  # Aggiungiamo tre parole sarcastiche
        # 2. Inserimento di una frase sarcastica completa
        text_with_insertion_2 = text + " " + sarcastic_phrases[0]  # Aggiungiamo una frase sarcastica

        # Tokenizzazione dei testi perturbati
        inputs = tokenizer([text_with_insertion], padding=True, truncation=True, return_tensors="pt")
        inputs = {key: value.to(device) for key, value in inputs.items()}

        # Classificazione del testo con il primo tipo di perturbazione
        with torch.no_grad():
            outputs = model(**inputs)
        logits = outputs.logits
        probabilities = F.softmax(logits, dim=-1)
        predicted_class = torch.argmax(probabilities, dim=-1).tolist()[0]

        # Stampa la previsione dopo il primo tipo di perturbazione
        print(f"Text with Insertion (sarcastic words): \"{text_with_insertion}\" -> Predicted Class: {class_labels[predicted_class]} (Probability: {probabilities[0][predicted_class].item():.4f})")

        # Tokenizzazione del secondo tipo di testo perturbato (con frase sarcastica completa)
        inputs = tokenizer([text_with_insertion_2], padding=True, truncation=True, return_tensors="pt")
        inputs = {key: value.to(device) for key, value in inputs.items()}

        # Classificazione del testo con il secondo tipo di perturbazione
        with torch.no_grad():
            outputs = model(**inputs)
        logits = outputs.logits
        probabilities = F.softmax(logits, dim=-1)
        predicted_class = torch.argmax(probabilities, dim=-1).tolist()[0]

        # Stampa la previsione dopo il secondo tipo di perturbazione
        print(f"Text with Insertion (sarcastic phrase): \"{text_with_insertion_2}\" -> Predicted Class: {class_labels[predicted_class]} (Probability: {probabilities[0][predicted_class].item():.4f})")
        print("-" * 50)

# Esempi di recensioni sarcastiche da analizzare
texts = [
    "I really dislike that people don't like this movie.",
    "The film was a complete waste of time.",
    "This movie was a disaster, absolutely brilliant!",
    "What a borefest this movie was. So long and dragged on."
]

# Esegui l'attacco sarcastico
sarcasm_insertion_attack(texts)